{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT - finetuning for classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Toy-Proto/envcommon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, AutoTokenizer,BertConfig\n",
    "from torch.optim import Adam \n",
    "\n",
    "base_model=BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "NUM_OF_CLASSES=3\n",
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_OF_EPOCHS=20\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the collate_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_func(list_of_data_points):\n",
    "    \n",
    "    tokenized_texts=tokenizer([x[\"text\"] for x in list_of_data_points],\n",
    "                              max_length=128,\n",
    "                              truncation=True,\n",
    "                              padding=\"max_length\",\n",
    "                              return_tensors=\"pt\"\n",
    "                              )\n",
    "    return {\"input_ids\":tokenized_texts[\"input_ids\"],\n",
    "            \"attention_mask\":tokenized_texts[\"attention_mask\"],\n",
    "            'token_type_ids':tokenized_texts[\"token_type_ids\"],\n",
    "            \"label\": torch.tensor([x[\"label\"] for x in list_of_data_points],dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# sample data\n",
    "ds = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader=DataLoader(dataset=ds[\"train\"],batch_size=16,collate_fn=collate_func)\n",
    "val_dataloader=DataLoader(dataset=ds[\"validation\"],batch_size=16,collate_fn=collate_func)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WrapperModel(nn.Module):\n",
    "    def __init__(self,) -> None:\n",
    "        super().__init__()\n",
    "        self.base_model=BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        self.classification_head=nn.Linear(BertConfig().hidden_size,NUM_OF_CLASSES)\n",
    "        for param in self.base_model.parameters(): # Freezing the base model parameters\n",
    "            param.requires_grad=False\n",
    "        \n",
    "    \n",
    "    def forward(self,input_ids,attention_masks,token_type_ids):\n",
    "        x=self.base_model(input_ids,attention_masks,token_type_ids)[\"pooler_output\"]\n",
    "        x=F.relu(x)\n",
    "        logits=self.classification_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=WrapperModel()\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:35<00:00, 20.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.064557820758935 at epoch_number 1\n",
      "\n",
      "\n",
      " On the Validation Dataset \n",
      "confusion_matrix\n",
      "[[ 106 1352   59]\n",
      " [  31 1779  118]\n",
      " [  10 1198  552]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:42<00:00, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.06021692003428814 at epoch_number 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:47<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.0584824707160597 at epoch_number 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:47<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.057577101593898214 at epoch_number 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:47<00:00, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.0568378423874984 at epoch_number 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:46<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.056513312522734165 at epoch_number 6\n",
      "\n",
      "\n",
      " On the Validation Dataset \n",
      "confusion_matrix\n",
      "[[ 412  988  117]\n",
      " [ 136 1513  279]\n",
      " [  48  754  958]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:46<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.056132759261479384 at epoch_number 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:47<00:00, 18.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.055921754215798175 at epoch_number 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:46<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.0556175043867504 at epoch_number 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:47<00:00, 18.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.05539915186723267 at epoch_number 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [01:47<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss of 0.055372260415499086 at epoch_number 11\n",
      "\n",
      "\n",
      " On the Validation Dataset \n",
      "confusion_matrix\n",
      "[[ 445  954  118]\n",
      " [ 137 1477  314]\n",
      " [  51  684 1025]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 633/1952 [00:34<01:12, 18.22it/s]"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "model=model.to(DEVICE)\n",
    "\n",
    "for epoch_id in range(NUM_OF_EPOCHS):\n",
    "    tot_loss=0\n",
    "    tot_count=0\n",
    "    model.train()\n",
    "    for data in tqdm(train_dataloader):\n",
    "        input_ids,attention_masks,token_type_ids,label=data[\"input_ids\"].to(DEVICE),data[\"attention_mask\"].to(DEVICE),data[\"token_type_ids\"].to(DEVICE),data[\"label\"].to(DEVICE)\n",
    "        logits=model(input_ids,attention_masks,token_type_ids)\n",
    "        loss=loss_func(logits,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tot_loss+=loss.item()\n",
    "        tot_count+=label.shape[0]\n",
    "        \n",
    "    print(f\"avg training loss of {tot_loss/tot_count} at epoch_number {epoch_id+1}\")\n",
    "\n",
    "    if epoch_id%5==0:\n",
    "        model.eval()\n",
    "        all_pred,all_truths=[],[]\n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                input_ids,attention_masks,token_type_ids,label=data[\"input_ids\"].to(DEVICE),data[\"attention_mask\"].to(DEVICE),data[\"token_type_ids\"].to(DEVICE),data[\"label\"].to(DEVICE)\n",
    "                logits=model(input_ids,attention_masks,token_type_ids)\n",
    "                predicted=torch.argmax(logits,dim=-1).view(-1).tolist()\n",
    "                truth=label.view(-1).tolist()\n",
    "                all_pred.extend(predicted)\n",
    "                all_truths.extend(truth)\n",
    "            conf_array=confusion_matrix(all_truths,all_pred)\n",
    "            live_metrics=classification_report(all_truths,all_pred)\n",
    "            print(\"\\n\\n On the Validation Dataset \")\n",
    "            print(\"confusion_matrix\")\n",
    "            print(conf_array)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envcommon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
